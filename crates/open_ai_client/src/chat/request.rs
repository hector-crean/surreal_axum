use serde::{Deserialize, Serialize};
use strum::{EnumString, EnumVariantNames};

use crate::{
    function::{Function, FunctionCall, FunctionCallType},
    role::OpenAiRole,
};

#[derive(Serialize, Deserialize, Debug)]
pub struct ChatCompletionRequest {
    ///A list of messages comprising the conversation so far
    messages: Vec<ChatCompletionMessage>,
    ///ID of the model to use.
    model: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    frequency_penalty: Option<f64>,
    /// Controls how the model calls functions. "none" means the model will not call a function and instead generates a message.
    /// "auto" means the model can pick between generating a message or calling a function. Specifying a particular function
    /// via {"name": "my_function"} forces the model to call that function. "none" is the default when no functions are present.
    /// "auto" is the default if functions are present.
    #[serde(skip_serializing_if = "Option::is_none")]
    function_call: Option<FunctionCallType>,
    ///A list of functions the model may generate JSON inputs for.
    #[serde(skip_serializing_if = "Option::is_none")]
    functions: Option<Vec<Function>>,
    /// Modify the likelihood of specified tokens appearing in the completion.
    /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
    /// Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but
    /// values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or
    /// exclusive selection of the relevant token.
    #[serde(skip_serializing_if = "Option::is_none")]
    logit_bias: Option<serde_json::Map<String, serde_json::Value>>,
    /// The maximum number of tokens to generate in the chat completion.
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<i32>,
    /// How many chat completion choices to generate for each input message.
    #[serde(skip_serializing_if = "Option::is_none")]
    n: Option<i32>,
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's
    /// likelihood to talk about new topics.
    #[serde(skip_serializing_if = "Option::is_none")]
    presence_penalty: Option<f64>,
    ///Up to 4 sequences where the API will stop generating further tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    stop: Option<StopOption>,
    /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available,
    /// with the stream terminated by a data: [DONE] message. Example Python code.
    #[serde(skip_serializing_if = "Option::is_none")]
    stream: Option<bool>,
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2
    /// will make it more focused and deterministic
    ///
    ///We generally recommend altering this or top_p but not both.
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
    /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    ///
    /// We generally recommend altering this or temperature but not both.
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f64>,
    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse
    #[serde(skip_serializing_if = "Option::is_none")]
    user: Option<String>,
}
impl ChatCompletionRequest {
    /// Creates a new `ChatCompletionRequest` instance with the given model and messages.
    pub fn new(model: String, messages: Vec<ChatCompletionMessage>) -> Self {
        Self {
            model,
            messages,
            // Set all optional fields to None.
            frequency_penalty: None,
            function_call: None,
            functions: None,
            logit_bias: None,
            max_tokens: None,
            n: None,
            presence_penalty: None,
            stop: None,
            stream: None,
            temperature: None,
            top_p: None,
            user: None,
        }
    }

    /// Sets the `frequency_penalty` field and returns the modified `ChatCompletionRequest`.
    pub fn frequency_penalty(mut self, value: f64) -> Self {
        self.frequency_penalty = Some(value);
        self
    }

    /// Sets the `function_call` field and returns the modified `ChatCompletionRequest`.
    pub fn function_call(mut self, value: FunctionCallType) -> Self {
        self.function_call = Some(value);
        self
    }

    /// Sets the `functions` field and returns the modified `ChatCompletionRequest`.
    pub fn functions(mut self, value: Vec<Function>) -> Self {
        self.functions = Some(value);
        self
    }

    /// Sets the `logit_bias` field and returns the modified `ChatCompletionRequest`.
    pub fn logit_bias(mut self, value: serde_json::Map<String, serde_json::Value>) -> Self {
        self.logit_bias = Some(value);
        self
    }

    /// Sets the `max_tokens` field and returns the modified `ChatCompletionRequest`.
    pub fn max_tokens(mut self, value: i32) -> Self {
        self.max_tokens = Some(value);
        self
    }

    /// Sets the `n` field and returns the modified `ChatCompletionRequest`.
    pub fn n(mut self, value: i32) -> Self {
        self.n = Some(value);
        self
    }

    /// Sets the `presence_penalty` field and returns the modified `ChatCompletionRequest`.
    pub fn presence_penalty(mut self, value: f64) -> Self {
        self.presence_penalty = Some(value);
        self
    }

    /// Sets the `stop` field and returns the modified `ChatCompletionRequest`.
    pub fn stop(mut self, value: StopOption) -> Self {
        self.stop = Some(value);
        self
    }

    /// Sets the `stream` field and returns the modified `ChatCompletionRequest`.
    pub fn stream(mut self, value: bool) -> Self {
        self.stream = Some(value);
        self
    }

    /// Sets the `temperature` field and returns the modified `ChatCompletionRequest`.
    pub fn temperature(mut self, value: f64) -> Self {
        self.temperature = Some(value);
        self
    }

    /// Sets the `top_p` field and returns the modified `ChatCompletionRequest`.
    pub fn top_p(mut self, value: f64) -> Self {
        self.top_p = Some(value);
        self
    }

    /// Sets the `user` field and returns the modified `ChatCompletionRequest`.
    pub fn user(mut self, value: String) -> Self {
        self.user = Some(value);
        self
    }
}

#[derive(Serialize, Deserialize, Debug)]
pub struct ChatCompletionMessage {
    /// The contents of the message. `content` is required for all messages, and may be null for assistant messages with function calls.
    #[serde(rename = "content", deserialize_with = "Option::deserialize")]
    pub content: Option<String>,
    #[serde(rename = "function_call", skip_serializing_if = "Option::is_none")]
    pub function_call: Option<FunctionCall>,
    /// The name of the author of this message. `name` is required if role is `function`, and it should be the name of the function whose response is in the `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.
    #[serde(rename = "name", skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The role of the messages author. One of `system`, `user`, `assistant`, or `function`.
    #[serde(rename = "role")]
    pub role: OpenAiRole,
}

impl ChatCompletionMessage {
    /// Creates a new `ChatCompletionMessage` instance with the given content and role.
    pub fn new(content: Option<String>, role: OpenAiRole) -> Self {
        Self {
            content,
            role,
            // Set all optional fields to None.
            function_call: None,
            name: None,
        }
    }

    /// Sets the `function_call` field and returns the modified `ChatCompletionMessage`.
    pub fn function_call(mut self, value: FunctionCall) -> Self {
        self.function_call = Some(value);
        self
    }

    /// Sets the `name` field and returns the modified `ChatCompletionMessage`.
    pub fn name(mut self, value: String) -> Self {
        self.name = Some(value);
        self
    }
}

#[derive(Serialize, Deserialize, Debug)]
pub enum StopOption {
    String(String),
    Array(Vec<String>),
}

impl StopOption {
    /// Creates a new `StopOption` instance from a single string.
    pub fn from_string(value: String) -> Self {
        Self::String(value)
    }

    /// Creates a new `StopOption` instance from a vector of strings.
    pub fn from_array(value: Vec<String>) -> Self {
        Self::Array(value)
    }
}
